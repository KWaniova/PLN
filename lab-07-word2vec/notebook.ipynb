{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfeZuPw4dW5j"
   },
   "source": [
    "# Lab 6 - word-2-vec with pytorch and gensim\n",
    "\n",
    " \"A word is characterized by the company it keeps\" - Firth (1957)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execise 0 (0pt)\n",
    "\n",
    "\n",
    "To do the following exercises you will need certain python packages. This\n",
    "first exercise is about installing them. You will need `sklearn`, `nltk`, `numpy`,\n",
    "`gensim`. Please make sure you have installed them (by your distribution’s\n",
    "package manager, pip, anaconda, . . . ) and check your installation by trying\n",
    "to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sklearn\n",
    "import sklearn\n",
    "import nltk\n",
    "import numpy\n",
    "# %pip install gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 (0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `wordspace.py` you find some convenience functions to extract a word\n",
    "cooccurrence matrix from text. Run the following script and evaluate the\n",
    "embeddings by looking at the nearest neighbors of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krystynawaniova/opt/miniconda3/envs/mldd23/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1379: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'and': 4,\n",
       " 'to': 5,\n",
       " 'a': 6,\n",
       " 'in': 7,\n",
       " 'that': 8,\n",
       " 'is': 9,\n",
       " 'was': 10,\n",
       " \"''\": 11,\n",
       " 'for': 12,\n",
       " '``': 13,\n",
       " 'with': 14,\n",
       " 'The': 15,\n",
       " 'it': 16,\n",
       " 'he': 17,\n",
       " 'as': 18,\n",
       " 'his': 19,\n",
       " 'on': 20,\n",
       " 'be': 21,\n",
       " 'I': 22,\n",
       " \"'s\": 23,\n",
       " '&': 24,\n",
       " 'had': 25,\n",
       " 'by': 26,\n",
       " 'at': 27,\n",
       " 'not': 28,\n",
       " 'are': 29,\n",
       " 'from': 30,\n",
       " 'or': 31,\n",
       " 'this': 32,\n",
       " 'have': 33,\n",
       " 'an': 34,\n",
       " 'which': 35,\n",
       " '*': 36,\n",
       " 'were': 37,\n",
       " '<': 38,\n",
       " '>': 39,\n",
       " 'but': 40,\n",
       " 'He': 41,\n",
       " 'you': 42,\n",
       " 'one': 43,\n",
       " 'her': 44,\n",
       " 'they': 45,\n",
       " 'would': 46,\n",
       " ';': 47,\n",
       " 'all': 48,\n",
       " '#': 49,\n",
       " 'their': 50,\n",
       " 'him': 51,\n",
       " 'been': 52,\n",
       " 'has': 53,\n",
       " ')': 54,\n",
       " '(': 55,\n",
       " '?': 56,\n",
       " 'who': 57,\n",
       " 'will': 58,\n",
       " 'It': 59,\n",
       " 'more': 60,\n",
       " \"n't\": 61,\n",
       " 'she': 62,\n",
       " 'we': 63,\n",
       " 'out': 64,\n",
       " 'can': 65,\n",
       " 'said': 66,\n",
       " 'there': 67,\n",
       " 'up': 68,\n",
       " 'than': 69,\n",
       " 'its': 70,\n",
       " 'into': 71,\n",
       " 'no': 72,\n",
       " 'them': 73,\n",
       " 'about': 74,\n",
       " 'so': 75,\n",
       " 'could': 76,\n",
       " 'when': 77,\n",
       " 'In': 78,\n",
       " ':': 79,\n",
       " 'only': 80,\n",
       " 'other': 81,\n",
       " 'do': 82,\n",
       " 'time': 83,\n",
       " 'if': 84,\n",
       " 'what': 85,\n",
       " 'A': 86,\n",
       " 'some': 87,\n",
       " 'did': 88,\n",
       " 'But': 89,\n",
       " 'two': 90,\n",
       " 'any': 91,\n",
       " 'may': 92,\n",
       " 'man': 93,\n",
       " 'first': 94,\n",
       " 'like': 95,\n",
       " 'these': 96,\n",
       " 'over': 97,\n",
       " 'such': 98,\n",
       " 'This': 99,\n",
       " 'my': 100,\n",
       " 'me': 101,\n",
       " 'our': 102,\n",
       " 'made': 103,\n",
       " 'most': 104,\n",
       " 'new': 105,\n",
       " 'now': 106,\n",
       " 'then': 107,\n",
       " 'must': 108,\n",
       " 'also': 109,\n",
       " 'even': 110,\n",
       " 'back': 111,\n",
       " 'before': 112,\n",
       " 'years': 113,\n",
       " 'through': 114,\n",
       " 'many': 115,\n",
       " 'There': 116,\n",
       " 'They': 117,\n",
       " 'much': 118,\n",
       " 'She': 119,\n",
       " 'way': 120,\n",
       " 'should': 121,\n",
       " 'down': 122,\n",
       " 'f': 123,\n",
       " 'your': 124,\n",
       " \"'\": 125,\n",
       " 'And': 126,\n",
       " 'where': 127,\n",
       " 'after': 128,\n",
       " 'Mr': 129,\n",
       " 'people': 130,\n",
       " 'because': 131,\n",
       " '!': 132,\n",
       " 'too': 133,\n",
       " 'little': 134,\n",
       " 'those': 135,\n",
       " 'very': 136,\n",
       " 'own': 137,\n",
       " 'make': 138,\n",
       " 'We': 139,\n",
       " 'good': 140,\n",
       " 'each': 141,\n",
       " 'well': 142,\n",
       " 'just': 143,\n",
       " 'work': 144,\n",
       " 'men': 145,\n",
       " 'still': 146,\n",
       " 'see': 147,\n",
       " 'If': 148,\n",
       " 'between': 149,\n",
       " 'get': 150,\n",
       " 'long': 151,\n",
       " 'world': 152,\n",
       " 'being': 153,\n",
       " 'same': 154,\n",
       " 'year': 155,\n",
       " 'know': 156,\n",
       " 'life': 157,\n",
       " 'might': 158,\n",
       " 'never': 159,\n",
       " 'us': 160,\n",
       " 'under': 161,\n",
       " 'both': 162,\n",
       " 'last': 163,\n",
       " 'day': 164,\n",
       " 'You': 165,\n",
       " 'off': 166,\n",
       " 'how': 167,\n",
       " 'h': 168,\n",
       " 'came': 169,\n",
       " 'against': 170,\n",
       " 'used': 171,\n",
       " 'great': 172,\n",
       " 'go': 173,\n",
       " 'For': 174,\n",
       " 'here': 175,\n",
       " 'right': 176,\n",
       " 'himself': 177,\n",
       " 'come': 178,\n",
       " 'few': 179,\n",
       " 'When': 180,\n",
       " 'take': 181,\n",
       " 'another': 182,\n",
       " 'old': 183,\n",
       " 'American': 184,\n",
       " 'state': 185,\n",
       " 'while': 186,\n",
       " 'use': 187,\n",
       " '$': 188,\n",
       " 'around': 189,\n",
       " 'New': 190,\n",
       " 'three': 191,\n",
       " 'As': 192,\n",
       " 'without': 193,\n",
       " 'found': 194,\n",
       " 'His': 195,\n",
       " 'Mrs': 196,\n",
       " 'place': 197,\n",
       " 'again': 198,\n",
       " '_': 199,\n",
       " 'home': 200,\n",
       " 'small': 201,\n",
       " 'does': 202,\n",
       " 'thought': 203,\n",
       " 'went': 204,\n",
       " 'say': 205,\n",
       " 'What': 206,\n",
       " '~': 207,\n",
       " 'got': 208,\n",
       " 'upon': 209,\n",
       " 'left': 210,\n",
       " 'number': 211,\n",
       " 'part': 212,\n",
       " 'course': 213,\n",
       " 'high': 214,\n",
       " 'United': 215,\n",
       " 'since': 216,\n",
       " 'during': 217,\n",
       " 'That': 218,\n",
       " 'always': 219,\n",
       " 'away': 220,\n",
       " 'fact': 221,\n",
       " '@': 222,\n",
       " 'States': 223,\n",
       " \"'ll\": 224,\n",
       " 'every': 225,\n",
       " 'until': 226,\n",
       " 'water': 227,\n",
       " 'think': 228,\n",
       " 'less': 229,\n",
       " 'took': 230,\n",
       " 'put': 231,\n",
       " 'enough': 232,\n",
       " 'head': 233,\n",
       " 'something': 234,\n",
       " 'school': 235,\n",
       " 'once': 236,\n",
       " 'hand': 237,\n",
       " 'One': 238,\n",
       " 'far': 239,\n",
       " 'night': 240,\n",
       " 'told': 241,\n",
       " 'almost': 242,\n",
       " 'public': 243,\n",
       " 'set': 244,\n",
       " 'At': 245,\n",
       " 'No': 246,\n",
       " \"'d\": 247,\n",
       " 'better': 248,\n",
       " 'house': 249,\n",
       " 'end': 250,\n",
       " 'called': 251,\n",
       " 'find': 252,\n",
       " 'asked': 253,\n",
       " 'knew': 254,\n",
       " 'going': 255,\n",
       " 'system': 256,\n",
       " 'eyes': 257,\n",
       " 'give': 258,\n",
       " 'toward': 259,\n",
       " 'however': 260,\n",
       " 'group': 261,\n",
       " 'To': 262,\n",
       " 'though': 263,\n",
       " 'given': 264,\n",
       " 'days': 265,\n",
       " 'possible': 266,\n",
       " 'present': 267,\n",
       " 'per': 268,\n",
       " 'point': 269,\n",
       " 'side': 270,\n",
       " 'face': 271,\n",
       " 'looked': 272,\n",
       " 'look': 273,\n",
       " 'program': 274,\n",
       " 'John': 275,\n",
       " 'room': 276,\n",
       " 'important': 277,\n",
       " 'next': 278,\n",
       " 'become': 279,\n",
       " 'young': 280,\n",
       " 'things': 281,\n",
       " 'THE': 282,\n",
       " 'order': 283,\n",
       " 'felt': 284,\n",
       " 'nothing': 285,\n",
       " 'children': 286,\n",
       " 'social': 287,\n",
       " '1': 288,\n",
       " 'business': 289,\n",
       " 'rather': 290,\n",
       " 'later': 291,\n",
       " 'Then': 292,\n",
       " 'large': 293,\n",
       " 'saw': 294,\n",
       " 'case': 295,\n",
       " 'form': 296,\n",
       " 'need': 297,\n",
       " 'often': 298,\n",
       " 'These': 299,\n",
       " 'least': 300,\n",
       " 'along': 301,\n",
       " 'second': 302,\n",
       " 'best': 303,\n",
       " 'several': 304,\n",
       " 'S': 305,\n",
       " 'ever': 306,\n",
       " 'seemed': 307,\n",
       " 'On': 308,\n",
       " 'early': 309,\n",
       " '}': 310,\n",
       " '{': 311,\n",
       " 'want': 312,\n",
       " 'God': 313,\n",
       " 'thing': 314,\n",
       " 'four': 315,\n",
       " 'mind': 316,\n",
       " 'power': 317,\n",
       " 'interest': 318,\n",
       " 'within': 319,\n",
       " 'country': 320,\n",
       " 'turned': 321,\n",
       " 'let': 322,\n",
       " 'area': 323,\n",
       " 'U': 324,\n",
       " 'light': 325,\n",
       " 'done': 326,\n",
       " 'among': 327,\n",
       " 'general': 328,\n",
       " '-': 329,\n",
       " 'began': 330,\n",
       " 'members': 331,\n",
       " 'big': 332,\n",
       " 'family': 333,\n",
       " 'sense': 334,\n",
       " 'door': 335,\n",
       " 'kind': 336,\n",
       " 'development': 337,\n",
       " 'problem': 338,\n",
       " 'different': 339,\n",
       " 'whole': 340,\n",
       " 'matter': 341,\n",
       " 'open': 342,\n",
       " 'certain': 343,\n",
       " 'itself': 344,\n",
       " 'war': 345,\n",
       " 'York': 346,\n",
       " 'help': 347,\n",
       " 'human': 348,\n",
       " 'means': 349,\n",
       " 'others': 350,\n",
       " 'gave': 351,\n",
       " 'action': 352,\n",
       " \"'re\": 353,\n",
       " 'example': 354,\n",
       " 'yet': 355,\n",
       " '2': 356,\n",
       " 'line': 357,\n",
       " 'feet': 358,\n",
       " 'hands': 359,\n",
       " 'President': 360,\n",
       " 'today': 361,\n",
       " 'name': 362,\n",
       " 'taken': 363,\n",
       " 'past': 364,\n",
       " 'city': 365,\n",
       " 'law': 366,\n",
       " 'seen': 367,\n",
       " 'local': 368,\n",
       " 'above': 369,\n",
       " 'car': 370,\n",
       " 'across': 371,\n",
       " 'All': 372,\n",
       " 'quite': 373,\n",
       " 'government': 374,\n",
       " 'either': 375,\n",
       " 'body': 376,\n",
       " 'themselves': 377,\n",
       " 'show': 378,\n",
       " 'State': 379,\n",
       " \"'m\": 380,\n",
       " 'experience': 381,\n",
       " 'With': 382,\n",
       " 'really': 383,\n",
       " 'Now': 384,\n",
       " 'history': 385,\n",
       " 'half': 386,\n",
       " 'anything': 387,\n",
       " 'words': 388,\n",
       " 'death': 389,\n",
       " 'C': 390,\n",
       " 'period': 391,\n",
       " 'times': 392,\n",
       " 'OF': 393,\n",
       " 'week': 394,\n",
       " 'whether': 395,\n",
       " 'word': 396,\n",
       " 'information': 397,\n",
       " 'money': 398,\n",
       " 'together': 399,\n",
       " 'having': 400,\n",
       " 'already': 401,\n",
       " 'political': 402,\n",
       " 'held': 403,\n",
       " 'real': 404,\n",
       " 'shall': 405,\n",
       " 'brought': 406,\n",
       " 'making': 407,\n",
       " 'seems': 408,\n",
       " 'white': 409,\n",
       " 'whose': 410,\n",
       " 'boy': 411,\n",
       " 'moment': 412,\n",
       " 'became': 413,\n",
       " 'After': 414,\n",
       " 'tell': 415,\n",
       " 'known': 416,\n",
       " 'question': 417,\n",
       " 'behind': 418,\n",
       " 'free': 419,\n",
       " 'keep': 420,\n",
       " \"'ve\": 421,\n",
       " 'result': 422,\n",
       " 'ago': 423,\n",
       " 'five': 424,\n",
       " 'field': 425,\n",
       " 'heard': 426,\n",
       " 'service': 427,\n",
       " 'individual': 428,\n",
       " 'why': 429,\n",
       " 'child': 430,\n",
       " 'available': 431,\n",
       " 'wife': 432,\n",
       " 'study': 433,\n",
       " 'sure': 434,\n",
       " 'company': 435,\n",
       " 'problems': 436,\n",
       " 'position': 437,\n",
       " 'reason': 438,\n",
       " 'probably': 439,\n",
       " 'areas': 440,\n",
       " 'change': 441,\n",
       " 'Miss': 442,\n",
       " 'job': 443,\n",
       " 'close': 444,\n",
       " 'woman': 445,\n",
       " 'seem': 446,\n",
       " 'So': 447,\n",
       " 'special': 448,\n",
       " 'turn': 449,\n",
       " 'full': 450,\n",
       " 'major': 451,\n",
       " 'wanted': 452,\n",
       " 'am': 453,\n",
       " 'mother': 454,\n",
       " 'church': 455,\n",
       " 'voice': 456,\n",
       " 'cost': 457,\n",
       " 'clear': 458,\n",
       " 'economic': 459,\n",
       " 'air': 460,\n",
       " 'necessary': 461,\n",
       " 'front': 462,\n",
       " 'girl': 463,\n",
       " 'feel': 464,\n",
       " 'age': 465,\n",
       " 'provide': 466,\n",
       " 'able': 467,\n",
       " 'true': 468,\n",
       " 'policy': 469,\n",
       " 'effect': 470,\n",
       " 'office': 471,\n",
       " 'community': 472,\n",
       " 'America': 473,\n",
       " 'love': 474,\n",
       " 'perhaps': 475,\n",
       " 'town': 476,\n",
       " 'stood': 477,\n",
       " 'future': 478,\n",
       " 'national': 479,\n",
       " 'rate': 480,\n",
       " 'level': 481,\n",
       " 'common': 482,\n",
       " 'morning': 483,\n",
       " 'run': 484,\n",
       " 'students': 485,\n",
       " 'following': 486,\n",
       " 'total': 487,\n",
       " 'Some': 488,\n",
       " 'control': 489,\n",
       " 'million': 490,\n",
       " 'short': 491,\n",
       " 'By': 492,\n",
       " 'evidence': 493,\n",
       " 'sound': 494,\n",
       " 'play': 495,\n",
       " 'top': 496,\n",
       " 'party': 497,\n",
       " 'believe': 498,\n",
       " 'although': 499,\n",
       " 'hard': 500,\n",
       " 'mean': 501,\n",
       " 'How': 502,\n",
       " 'surface': 503,\n",
       " 'Washington': 504,\n",
       " 'value': 505,\n",
       " 'Do': 506,\n",
       " '3': 507,\n",
       " 'situation': 508,\n",
       " 'women': 509,\n",
       " 'leave': 510,\n",
       " 'father': 511,\n",
       " 'six': 512,\n",
       " 'various': 513,\n",
       " 'military': 514,\n",
       " 'strong': 515,\n",
       " 'force': 516,\n",
       " 'says': 517,\n",
       " 'English': 518,\n",
       " 'lines': 519,\n",
       " 'started': 520,\n",
       " 'land': 521,\n",
       " 'society': 522,\n",
       " 'plan': 523,\n",
       " 'increase': 524,\n",
       " 'longer': 525,\n",
       " 'further': 526,\n",
       " 'An': 527,\n",
       " 'idea': 528,\n",
       " 'months': 529,\n",
       " 'type': 530,\n",
       " 'minutes': 531,\n",
       " 'music': 532,\n",
       " 'near': 533,\n",
       " 'personal': 534,\n",
       " 'gone': 535,\n",
       " 'usually': 536,\n",
       " 'center': 537,\n",
       " 'House': 538,\n",
       " 'Dr': 539,\n",
       " 'outside': 540,\n",
       " 'alone': 541,\n",
       " 'process': 542,\n",
       " 'road': 543,\n",
       " 'Even': 544,\n",
       " 'Not': 545,\n",
       " 'schools': 546,\n",
       " 'kept': 547,\n",
       " 'West': 548,\n",
       " 'ground': 549,\n",
       " 'living': 550,\n",
       " 'nature': 551,\n",
       " 'basis': 552,\n",
       " 'private': 553,\n",
       " 'view': 554,\n",
       " 'century': 555,\n",
       " 'pressure': 556,\n",
       " 'expected': 557,\n",
       " 'tax': 558,\n",
       " 'required': 559,\n",
       " 'else': 560,\n",
       " 'General': 561,\n",
       " 'wrote': 562,\n",
       " 'modern': 563,\n",
       " 'moved': 564,\n",
       " 'values': 565,\n",
       " 'greater': 566,\n",
       " 'call': 567,\n",
       " 'South': 568,\n",
       " 'art': 569,\n",
       " 'soon': 570,\n",
       " 'return': 571,\n",
       " '1960': 572,\n",
       " 'particular': 573,\n",
       " 'book': 574,\n",
       " 'f.': 575,\n",
       " 'support': 576,\n",
       " 'conditions': 577,\n",
       " 'complete': 578,\n",
       " 'attention': 579,\n",
       " 'cut': 580,\n",
       " 'needed': 581,\n",
       " 'person': 582,\n",
       " 'late': 583,\n",
       " 'Department': 584,\n",
       " 'live': 585,\n",
       " 'hours': 586,\n",
       " 'Since': 587,\n",
       " 'lost': 588,\n",
       " 'followed': 589,\n",
       " 'board': 590,\n",
       " 'amount': 591,\n",
       " 'third': 592,\n",
       " 'except': 593,\n",
       " 'However': 594,\n",
       " 'fire': 595,\n",
       " 'including': 596,\n",
       " 'hundred': 597,\n",
       " 'tried': 598,\n",
       " 'single': 599,\n",
       " 'added': 600,\n",
       " 'education': 601,\n",
       " 'coming': 602,\n",
       " 'heart': 603,\n",
       " 'developed': 604,\n",
       " 'industry': 605,\n",
       " 'reached': 606,\n",
       " 'Of': 607,\n",
       " 'dark': 608,\n",
       " 'material': 609,\n",
       " 'stage': 610,\n",
       " 'space': 611,\n",
       " 'makes': 612,\n",
       " 'miles': 613,\n",
       " 'recent': 614,\n",
       " 'act': 615,\n",
       " 'shown': 616,\n",
       " 'feeling': 617,\n",
       " 'Thus': 618,\n",
       " 'move': 619,\n",
       " 'costs': 620,\n",
       " 'pay': 621,\n",
       " 'read': 622,\n",
       " 'hope': 623,\n",
       " 'simply': 624,\n",
       " 'taking': 625,\n",
       " 'Kennedy': 626,\n",
       " 'dead': 627,\n",
       " 'figure': 628,\n",
       " 'basic': 629,\n",
       " 'trying': 630,\n",
       " 'sometimes': 631,\n",
       " 'hold': 632,\n",
       " 'doing': 633,\n",
       " 'inside': 634,\n",
       " 'looking': 635,\n",
       " 'sort': 636,\n",
       " 'received': 637,\n",
       " 'states': 638,\n",
       " 'difficult': 639,\n",
       " 'equipment': 640,\n",
       " 'terms': 641,\n",
       " 'cold': 642,\n",
       " 'class': 643,\n",
       " 'care': 644,\n",
       " 'National': 645,\n",
       " 'spirit': 646,\n",
       " 'everything': 647,\n",
       " 'low': 648,\n",
       " 'beginning': 649,\n",
       " 'picture': 650,\n",
       " 'nor': 651,\n",
       " 'college': 652,\n",
       " 'subject': 653,\n",
       " 'Why': 654,\n",
       " 'From': 655,\n",
       " 'street': 656,\n",
       " 'getting': 657,\n",
       " 'floor': 658,\n",
       " 'My': 659,\n",
       " 'War': 660,\n",
       " 'walked': 661,\n",
       " 'passed': 662,\n",
       " 'religious': 663,\n",
       " 'especially': 664,\n",
       " 'bring': 665,\n",
       " 'rest': 666,\n",
       " 'black': 667,\n",
       " 'beyond': 668,\n",
       " 'simple': 669,\n",
       " 'cent': 670,\n",
       " 'St': 671,\n",
       " 'Secretary': 672,\n",
       " 'Government': 673,\n",
       " 'England': 674,\n",
       " 'range': 675,\n",
       " 'ca': 676,\n",
       " 'paper': 677,\n",
       " 'William': 678,\n",
       " '10': 679,\n",
       " 'written': 680,\n",
       " 'data': 681,\n",
       " 'meeting': 682,\n",
       " 'likely': 683,\n",
       " 'report': 684,\n",
       " 'talk': 685,\n",
       " '%': 686,\n",
       " '4': 687,\n",
       " 'considered': 688,\n",
       " 'friends': 689,\n",
       " 'Her': 690,\n",
       " 'Congress': 691,\n",
       " 'hear': 692,\n",
       " 'final': 693,\n",
       " 'growth': 694,\n",
       " 'therefore': 695,\n",
       " 'son': 696,\n",
       " 'property': 697,\n",
       " 'fine': 698,\n",
       " 'working': 699,\n",
       " 'higher': 700,\n",
       " 'nation': 701,\n",
       " 'countries': 702,\n",
       " 'table': 703,\n",
       " 'entire': 704,\n",
       " 'ten': 705,\n",
       " 'natural': 706,\n",
       " 'sat': 707,\n",
       " 'needs': 708,\n",
       " 'building': 709,\n",
       " 'difference': 710,\n",
       " 'answer': 711,\n",
       " 'husband': 712,\n",
       " 'Here': 713,\n",
       " 'forces': 714,\n",
       " 'thus': 715,\n",
       " 'section': 716,\n",
       " 'happened': 717,\n",
       " 'story': 718,\n",
       " 'hair': 719,\n",
       " 'D': 720,\n",
       " 'similar': 721,\n",
       " 'effort': 722,\n",
       " 'cases': 723,\n",
       " 'meet': 724,\n",
       " 'hour': 725,\n",
       " 'paid': 726,\n",
       " 'stand': 727,\n",
       " 'training': 728,\n",
       " 'purpose': 729,\n",
       " 'sent': 730,\n",
       " 'points': 731,\n",
       " 'Christian': 732,\n",
       " 'earlier': 733,\n",
       " 'ready': 734,\n",
       " 'whom': 735,\n",
       " 'involved': 736,\n",
       " 'knowledge': 737,\n",
       " 'North': 738,\n",
       " 'particularly': 739,\n",
       " 'issue': 740,\n",
       " 'decided': 741,\n",
       " 'statement': 742,\n",
       " 'showed': 743,\n",
       " 'increased': 744,\n",
       " 'weeks': 745,\n",
       " 'market': 746,\n",
       " 'letter': 747,\n",
       " 'results': 748,\n",
       " 'start': 749,\n",
       " 'production': 750,\n",
       " 'addition': 751,\n",
       " '+': 752,\n",
       " 'bad': 753,\n",
       " 'wall': 754,\n",
       " 'stock': 755,\n",
       " 'moral': 756,\n",
       " 'French': 757,\n",
       " 'summer': 758,\n",
       " 'directly': 759,\n",
       " 'due': 760,\n",
       " 'girls': 761,\n",
       " 'thinking': 762,\n",
       " 'B': 763,\n",
       " 'East': 764,\n",
       " 'ideas': 765,\n",
       " 'Yet': 766,\n",
       " 'Well': 767,\n",
       " '1961': 768,\n",
       " 'normal': 769,\n",
       " 'color': 770,\n",
       " 'concerned': 771,\n",
       " 'understand': 772,\n",
       " 'N': 773,\n",
       " 'appeared': 774,\n",
       " 'lay': 775,\n",
       " '/2': 776,\n",
       " 'population': 777,\n",
       " 'reading': 778,\n",
       " 'foreign': 779,\n",
       " 'comes': 780,\n",
       " 'nearly': 781,\n",
       " 'size': 782,\n",
       " 'merely': 783,\n",
       " 'police': 784,\n",
       " 'programs': 785,\n",
       " 'record': 786,\n",
       " 'trade': 787,\n",
       " 'member': 788,\n",
       " 'continued': 789,\n",
       " 'Brown': 790,\n",
       " 'City': 791,\n",
       " 'J': 792,\n",
       " 'questions': 793,\n",
       " 'using': 794,\n",
       " 'fall': 795,\n",
       " 'ran': 796,\n",
       " 'boys': 797,\n",
       " 'below': 798,\n",
       " 'opened': 799,\n",
       " 'trouble': 800,\n",
       " 'finally': 801,\n",
       " 'methods': 802,\n",
       " 'deal': 803,\n",
       " 'direction': 804,\n",
       " 'anyone': 805,\n",
       " 'peace': 806,\n",
       " 'food': 807,\n",
       " 'George': 808,\n",
       " 'During': 809,\n",
       " 'led': 810,\n",
       " 'president': 811,\n",
       " 'AND': 812,\n",
       " 'influence': 813,\n",
       " 'month': 814,\n",
       " 'suddenly': 815,\n",
       " 'M': 816,\n",
       " 'met': 817,\n",
       " 'Soviet': 818,\n",
       " 'method': 819,\n",
       " 'instead': 820,\n",
       " 'Federal': 821,\n",
       " 'list': 822,\n",
       " 'strength': 823,\n",
       " 'piece': 824,\n",
       " 'literature': 825,\n",
       " 'Let': 826,\n",
       " 'worked': 827,\n",
       " 'provided': 828,\n",
       " 'Island': 829,\n",
       " 'actually': 830,\n",
       " 'step': 831,\n",
       " 'chance': 832,\n",
       " 'stopped': 833,\n",
       " 'myself': 834,\n",
       " 'forms': 835,\n",
       " 'effective': 836,\n",
       " 'former': 837,\n",
       " 'University': 838,\n",
       " 'average': 839,\n",
       " 'wrong': 840,\n",
       " 'try': 841,\n",
       " 'Church': 842,\n",
       " 'friend': 843,\n",
       " 'temperature': 844,\n",
       " 'placed': 845,\n",
       " 'earth': 846,\n",
       " 'cause': 847,\n",
       " 'degree': 848,\n",
       " 'changes': 849,\n",
       " 'lead': 850,\n",
       " 'evening': 851,\n",
       " 'physical': 852,\n",
       " 'theory': 853,\n",
       " 'manner': 854,\n",
       " 'research': 855,\n",
       " 'defense': 856,\n",
       " 'efforts': 857,\n",
       " 'ways': 858,\n",
       " 'carried': 859,\n",
       " 'bed': 860,\n",
       " 'herself': 861,\n",
       " 'systems': 862,\n",
       " 'somewhat': 863,\n",
       " 'movement': 864,\n",
       " 'game': 865,\n",
       " 'lot': 866,\n",
       " 'red': 867,\n",
       " 'services': 868,\n",
       " 'couple': 869,\n",
       " 'sales': 870,\n",
       " 'series': 871,\n",
       " 'meaning': 872,\n",
       " 'performance': 873,\n",
       " 'latter': 874,\n",
       " 'wide': 875,\n",
       " 'respect': 876,\n",
       " 'throughout': 877,\n",
       " 'court': 878,\n",
       " 'freedom': 879,\n",
       " 'plant': 880,\n",
       " 'Although': 881,\n",
       " 'beautiful': 882,\n",
       " 'larger': 883,\n",
       " 'While': 884,\n",
       " 'aid': 885,\n",
       " 'direct': 886,\n",
       " 'described': 887,\n",
       " 'groups': 888,\n",
       " 'truth': 889,\n",
       " 'easy': 890,\n",
       " 'fear': 891,\n",
       " 'treatment': 892,\n",
       " 'charge': 893,\n",
       " 'reaction': 894,\n",
       " 'determined': 895,\n",
       " 'opportunity': 896,\n",
       " 'approach': 897,\n",
       " 'Europe': 898,\n",
       " 'hot': 899,\n",
       " 'reported': 900,\n",
       " 'served': 901,\n",
       " 'understanding': 902,\n",
       " 'clearly': 903,\n",
       " 'appear': 904,\n",
       " 'labor': 905,\n",
       " 'running': 906,\n",
       " 'remember': 907,\n",
       " 'works': 908,\n",
       " 'decision': 909,\n",
       " '5': 910,\n",
       " 'lower': 911,\n",
       " 'numbers': 912,\n",
       " 'indeed': 913,\n",
       " 'window': 914,\n",
       " 'generally': 915,\n",
       " 'medical': 916,\n",
       " 'immediately': 917,\n",
       " 'trial': 918,\n",
       " 'Each': 919,\n",
       " 'certainly': 920,\n",
       " 'eye': 921,\n",
       " 'Just': 922,\n",
       " 'British': 923,\n",
       " 'persons': 924,\n",
       " 'based': 925,\n",
       " 'nations': 926,\n",
       " 'Act': 927,\n",
       " 'First': 928,\n",
       " 'character': 929,\n",
       " 'audience': 930,\n",
       " 'learned': 931,\n",
       " 'ask': 932,\n",
       " 'White': 933,\n",
       " 'returned': 934,\n",
       " 'obtained': 935,\n",
       " 'international': 936,\n",
       " 'image': 937,\n",
       " 'Christ': 938,\n",
       " 'Court': 939,\n",
       " '30': 940,\n",
       " 'technical': 941,\n",
       " 'responsibility': 942,\n",
       " 'forward': 943,\n",
       " '15': 944,\n",
       " 'account': 945,\n",
       " 'horse': 946,\n",
       " 'serious': 947,\n",
       " 'volume': 948,\n",
       " 'function': 949,\n",
       " 'activity': 950,\n",
       " 'industrial': 951,\n",
       " 'corner': 952,\n",
       " 'length': 953,\n",
       " 'straight': 954,\n",
       " 'blood': 955,\n",
       " 'steps': 956,\n",
       " 'lived': 957,\n",
       " 'ones': 958,\n",
       " '[': 959,\n",
       " 'saying': 960,\n",
       " 'Thomas': 961,\n",
       " 'gives': 962,\n",
       " 'hit': 963,\n",
       " 'plane': 964,\n",
       " 'h.': 965,\n",
       " 'pool': 966,\n",
       " 'federal': 967,\n",
       " 'test': 968,\n",
       " 'doubt': 969,\n",
       " 'writing': 970,\n",
       " ']': 971,\n",
       " 'types': 972,\n",
       " 'include': 973,\n",
       " 'farm': 974,\n",
       " 'according': 975,\n",
       " 'fiscal': 976,\n",
       " 'student': 977,\n",
       " 'nuclear': 978,\n",
       " 'completely': 979,\n",
       " 'Such': 980,\n",
       " 'quality': 981,\n",
       " 'pattern': 982,\n",
       " 'visit': 983,\n",
       " 'Another': 984,\n",
       " 'R': 985,\n",
       " 'choice': 986,\n",
       " 'firm': 987,\n",
       " 'cars': 988,\n",
       " 'arms': 989,\n",
       " 'moving': 990,\n",
       " 'stop': 991,\n",
       " 'recently': 992,\n",
       " 'slowly': 993,\n",
       " 'extent': 994,\n",
       " 'letters': 995,\n",
       " 'planning': 996,\n",
       " 'parts': 997,\n",
       " 'wish': 998,\n",
       " 'lack': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordspace import cooccurrence_matrix ,nearest_neighbor_loop\n",
    "\n",
    "with open('brown.txt', 'r') as f:\n",
    "    brown = f.read()\n",
    "\n",
    "matrix , vocabulary = cooccurrence_matrix(brown)\n",
    "vocabulary\n",
    "# len(vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "nearest_neighbor_loop(np.asarray(matrix) , vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix\n",
    "del vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to improve a basic counting model is transforming the word\n",
    "counts by, e.g., applying the square root afterwards.\n",
    "Modify the script from exercise 1.1 by using `numpy.sqrt` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye.\n"
     ]
    }
   ],
   "source": [
    "from wordspace import  nearest_neighbor_loop, cooccurrence_matrix\n",
    "import numpy as np\n",
    "\n",
    "with open('brown.txt', 'r') as f:\n",
    "    brown = f.read()\n",
    "\n",
    "matrix , vocabulary = cooccurrence_matrix(brown)\n",
    "matrix = np.sqrt(matrix)\n",
    "vocabulary\n",
    "\n",
    "nearest_neighbor_loop(np.asarray(matrix) , vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us examine the parameters of the function `cooccurrence_matrix`.\n",
    "You can modify the `window_size` and/or try a different vectorizer than\n",
    "the standard `CountVectorizer` to compute the cooccurrence scores. Try\n",
    "`sklearn.feature_extraction.text.TfidfVectorizer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "matrix , vocabulary = cooccurrence_matrix(\n",
    "    brown , window_size=5, max_vocab_size=20000,\n",
    "    same_word_zero=False , vectorizer=CountVectorizer\n",
    ")\n",
    "# matrix = np.sqrt(matrix)\n",
    "# vocabulary\n",
    "nearest_neighbor_loop(np.asarray(matrix) , vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "matrix , vocabulary = cooccurrence_matrix(\n",
    "    brown , window_size=3, max_vocab_size=20000,\n",
    "    same_word_zero=False , vectorizer=TfidfVectorizer\n",
    ")\n",
    "matrix = np.sqrt(matrix)\n",
    "vocabulary\n",
    "nearest_neighbor_loop(np.asarray(matrix) , vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Singular Value Decomposition (SVD) you can reduce the dimensionality\n",
    "of your embeddings. Try `sklearn.decomposition.TruncatedSVD` and\n",
    "see how your embeddings change! Consider the following usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m svd \u001b[38;5;241m=\u001b[39m TruncatedSVD(\n\u001b[1;32m     13\u001b[0m     n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m new_C \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mfit_transform(np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mC\u001b[49m))\n\u001b[1;32m     18\u001b[0m nearest_neighbor_loop(np\u001b[38;5;241m.\u001b[39masarray(new_C) , V)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "with open('brown.txt', 'r') as f:\n",
    "    brown = f.read()\n",
    "some_text = brown\n",
    "try:\n",
    "    C, V = cooccurrence_matrix(some_text)\n",
    "except:\n",
    "    print(\"Error\")\n",
    "\n",
    "svd = TruncatedSVD(\n",
    "    n_components=100, algorithm=\"randomized\",\n",
    "    n_iter=5, random_state=42, tol=0.\n",
    ")\n",
    "new_C = svd.fit_transform(np.asarray(C))\n",
    "\n",
    "nearest_neighbor_loop(np.asarray(new_C) , V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[139945,      0,      0, ...,      0,      0,      0],\n",
       "        [     0,      0,      0, ...,      0,      0,      0],\n",
       "        [     0,      0,      0, ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [     0,      0,      0, ...,      6,      0,      0],\n",
       "        [     0,      0,      0, ...,      0,      6,      0],\n",
       "        [     0,      0,      0, ...,      0,      0,      6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.40180987e+05, -9.78944357e+03, -1.85329998e+03, ...,\n",
       "         2.17947103e+00, -9.83446186e-01, -1.43103094e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 4.01801534e-02, -1.61822499e-02,  1.00006853e+00, ...,\n",
       "        -1.86342415e-03, -1.63230732e-03, -1.46320769e-03],\n",
       "       [ 1.13119133e-02, -2.68964775e-03,  2.06425080e-03, ...,\n",
       "         1.35455199e-04,  4.46409073e-04, -1.55133255e-03],\n",
       "       [ 4.69191491e-02, -1.80478421e-02,  9.99648854e-01, ...,\n",
       "        -9.27589662e-04, -7.97547423e-04, -2.00336248e-03]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C\n",
    "del new_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_components` - desired embedding dimension\n",
    "- `algorithm` - SVD solver to use; either “arpack” or “randomized”\n",
    "- `n_iter` - number of iterations for randomized SVD solver (not used by ARPACK)\n",
    "- `random_state` - seed for pseudo-random number generator\n",
    "- `tol` -  toleranze for ARPACK. Ignored by randomized SVD solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code snippets to train your own word2vec model on the\n",
    "brown corpus (or any other large text file you have). `semantic_tests.py`\n",
    "contains some tests for your embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 08:09:41,269: INFO: collecting all words and their counts\n",
      "2024-05-23 08:09:41,269: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-05-23 08:09:42,031: INFO: PROGRESS: at sentence #10000, processed 257804 words, keeping 25923 word types\n",
      "2024-05-23 08:09:42,751: INFO: PROGRESS: at sentence #20000, processed 502508 words, keeping 38331 word types\n",
      "2024-05-23 08:09:43,565: INFO: PROGRESS: at sentence #30000, processed 788265 words, keeping 47890 word types\n",
      "2024-05-23 08:09:44,241: INFO: PROGRESS: at sentence #40000, processed 993450 words, keeping 53867 word types\n",
      "2024-05-23 08:09:44,793: INFO: PROGRESS: at sentence #50000, processed 1156731 words, keeping 57747 word types\n",
      "2024-05-23 08:09:44,883: INFO: collected 58661 word types from a corpus of 1184239 raw words and 51328 sentences\n",
      "2024-05-23 08:09:44,883: INFO: Creating a fresh vocabulary\n",
      "2024-05-23 08:09:44,914: INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 15068 unique words (25.69% of original 58661, drops 43593)', 'datetime': '2024-05-23T08:09:44.914646', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-05-23 08:09:44,915: INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1115136 word corpus (94.16% of original 1184239, drops 69103)', 'datetime': '2024-05-23T08:09:44.915157', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-05-23 08:09:44,952: INFO: deleting the raw counts dictionary of 58661 items\n",
      "2024-05-23 08:09:44,953: INFO: sample=0.001 downsamples 44 most-common words\n",
      "2024-05-23 08:09:44,953: INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 802590.4437899959 word corpus (72.0%% of prior 1115136)', 'datetime': '2024-05-23T08:09:44.953627', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-05-23 08:09:45,017: INFO: estimated required memory for 15068 words and 100 dimensions: 19588400 bytes\n",
      "2024-05-23 08:09:45,018: INFO: resetting layer weights\n",
      "2024-05-23 08:09:45,027: INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-05-23T08:09:45.027180', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-05-23 08:09:45,027: INFO: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 15068 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-05-23T08:09:45.027789', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-05-23 08:09:45,029: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,029: INFO: EPOCH 0: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,030: WARNING: EPOCH 0: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,030: WARNING: EPOCH 0: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,031: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,032: INFO: EPOCH 1: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,032: WARNING: EPOCH 1: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,032: WARNING: EPOCH 1: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,033: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,035: INFO: EPOCH 2: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,035: WARNING: EPOCH 2: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,036: WARNING: EPOCH 2: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,036: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,038: INFO: EPOCH 3: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,038: WARNING: EPOCH 3: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,038: WARNING: EPOCH 3: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,040: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,040: INFO: EPOCH 4: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,041: WARNING: EPOCH 4: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,041: WARNING: EPOCH 4: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,042: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,043: INFO: EPOCH 5: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,043: WARNING: EPOCH 5: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,043: WARNING: EPOCH 5: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,044: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,045: INFO: EPOCH 6: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,046: WARNING: EPOCH 6: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,046: WARNING: EPOCH 6: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,047: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,051: INFO: EPOCH 7: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,052: WARNING: EPOCH 7: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,052: WARNING: EPOCH 7: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,056: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,058: INFO: EPOCH 8: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,059: WARNING: EPOCH 8: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,060: WARNING: EPOCH 8: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,064: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,065: INFO: EPOCH 9: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,066: WARNING: EPOCH 9: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,067: WARNING: EPOCH 9: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,070: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,071: INFO: EPOCH 10: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,072: WARNING: EPOCH 10: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,072: WARNING: EPOCH 10: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,074: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,074: INFO: EPOCH 11: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,075: WARNING: EPOCH 11: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,075: WARNING: EPOCH 11: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,076: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,077: INFO: EPOCH 12: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,077: WARNING: EPOCH 12: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,078: WARNING: EPOCH 12: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,080: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,081: INFO: EPOCH 13: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,081: WARNING: EPOCH 13: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,082: WARNING: EPOCH 13: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,083: WARNING: train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2024-05-23 08:09:45,083: INFO: EPOCH 14: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2024-05-23 08:09:45,083: WARNING: EPOCH 14: supplied example count (0) did not equal expected count (51328)\n",
      "2024-05-23 08:09:45,084: WARNING: EPOCH 14: supplied raw word count (0) did not equal expected count (1184239)\n",
      "2024-05-23 08:09:45,084: INFO: Word2Vec lifecycle event {'msg': 'training on 0 raw words (0 effective words) took 0.1s, 0 effective words/s', 'datetime': '2024-05-23T08:09:45.084468', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-05-23 08:09:45,084: INFO: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15068, vector_size=100, alpha=0.025>', 'datetime': '2024-05-23T08:09:45.084741', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among\n",
      "[('scared', 0.37241917848587036), ('contractual', 0.3398129940032959), ('Internal', 0.3379846215248108), ('displaying', 0.32876285910606384), ('syllables', 0.3277350664138794), ('exposure', 0.32741430401802063), ('outer', 0.3218686282634735), ('mates', 0.31806090474128723), ('Cathy', 0.3173461854457855), ('fed', 0.316450297832489)]\n",
      "Problems\n",
      "[('deputy', 0.35137489438056946), ('staffs', 0.33552974462509155), ('behavior', 0.32039764523506165), ('lash', 0.316434770822525), ('Industrial', 0.31565478444099426), ('pull', 0.3142879605293274), ('originality', 0.3142140805721283), ('usually', 0.3120686709880829), ('policies', 0.31155094504356384), ('effectively', 0.31140434741973877)]\n",
      "\n",
      "Semantic Tests!\n",
      "\n",
      "Man is to king as woman is to deputy, lengthy, radical.\n",
      "From breakfast, cereal, dinner and lunch -- breakfast does not match.\n",
      "Similarity!\n",
      "man -- woman: -0.025125857442617416\n",
      "man -- silver: 0.0936817079782486\n",
      "-0.025125857\n",
      "-0.10157965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3724192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_tests import semantic_tests\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(levelname)s: %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sent = nltk.data.load(\n",
    "    'tokenizers/punkt/english.pickle'\n",
    ")\n",
    "\n",
    "with open('brown.txt', 'r') as f:\n",
    "    sentences = sent.tokenize(f.read())\n",
    "    sentences = map(lambda s: word_tokenize(s), sentences)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences , vector_size=100, window=5,\n",
    "    min_count=5, hs=0, negative=5,\n",
    "    cbow_mean=1, epochs=15, workers=3\n",
    ")\n",
    "\n",
    "sims = model.wv.most_similar('among', topn=10)\n",
    "print(\"Among\")\n",
    "print(sims)\n",
    "sims = model.wv.most_similar('problems', topn=10)\n",
    "sims = model.wv.most_similar('woman', topn=10)\n",
    "print(\"Problems\")\n",
    "print(sims)\n",
    "\n",
    "semantic_tests(model.wv)\n",
    "\n",
    "print(model.wv.similarity(\"man\", \"woman\"))\n",
    "print(model.wv.similarity(\"problems\", \"solutions\"))\n",
    "model.wv.similarity(\"among\", \"scared\")\n",
    "\n",
    "# model.wv.most_similar(\n",
    "#         positive=['among', 'among'], negative=['alone'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training your own word2vec model, you can also download pretrained\n",
    "embeddings and load them into `gensim.` Are they doing better in\n",
    "your `semantic_tests`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular pre-trained option is the Google News dataset model, containing 300-dimensional embeddings for 3 millions words and phrases. Download the binary file ‘GoogleNews-vectors-negative300.bin’ (1.3 GB compressed) from https://code.google.com/archive/p/word2vec/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 19:24:56,301: INFO: loading projection weights from vectors.bin\n",
      "2024-05-22 19:25:16,064: INFO: KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-05-22T19:25:16.063671', 'gensim': '4.3.2', 'python': '3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:36:46) \\n[Clang 16.0.6 ]', 'platform': 'macOS-14.3-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Tests!\n",
      "\n",
      "Man is to king as woman is to queen, monarch, princess.\n",
      "From breakfast, cereal, dinner and lunch -- cereal does not match.\n",
      "Similarity!\n",
      "man -- woman: 0.7664012312889099\n",
      "man -- silver: 0.10574154555797577\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from semantic_tests import semantic_tests\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    'vectors.bin',\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "semantic_tests(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation i pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Au1y1zTzg5L1"
   },
   "source": [
    "## Exercise 4 (2pt)\n",
    "\n",
    "\n",
    "- Train word2vec skip-gram model on sentence \"the quick brown fox jumps over the lazy dog\". Assume context window = 2, embedding_dim = 5. No preprocessing apart from tokenization.\n",
    "- Compute model output probabilities for words \"lazy\" and \"dog\". If you have trained the model correctly, the output probabilities for word \"lazy\" should be higher for words \"over\", \"the\", \"dog\" (close to 1/3 each) and lower for other words (close to 0 each). For word \"dog\", the output probabilities should be higher for words, \"the\", \"lazy\" (close to 1/2 each) and lower for other words (close to 0 each). \n",
    "- Compute dot product between the vector of word \"dog\" and the vector of word \"lazy\" (could be representation of center vector and representation of context vector) and between \"dog\" and \"brown\". Which one is higher? Why?\n",
    "\n",
    "\n",
    "You can use this tutorial https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "\n",
    "Use pytorch (or tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOKM7MX0g779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1986c8030>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "\n",
    "# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-Fx_0DGjZDX"
   },
   "outputs": [],
   "source": [
    "sentence = \"the quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRnIu7hAB6sF"
   },
   "source": [
    "If our vocabulary is bigger, the word2vec model needs a LOT of data to obtain reasonable results. With this amount of data, the code needs to be optimized very well. Writing such code will be more suitable for a project instead of a simple exercise, therefore in the next exercise we will use [gensim](https://radimrehurek.com/gensim/), a library made for efficient training of word vectors.\n",
    "\n",
    "## * Exercise (2pt)\n",
    "\n",
    "- Use [gensim](https://radimrehurek.com/gensim/) to train a word2vec model on [OpinRank](http://kavita-ganesan.com/entity-ranking-data/). You can follow this [tutorial](https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3), but make sure you have used negative sampling.\n",
    "- Find 10 similar words to word \"dirty\" and \"canada\"\n",
    "- Check if similarity between \"dirty\" and \"dusty\" is bigger than between \"dirty\" and \"clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-WqD1xKDE0p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "lab-6-word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
