{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfeZuPw4dW5j"
   },
   "source": [
    "# Lab 6 - word-2-vec with pytorch and gensim\n",
    "\n",
    " \"A word is characterized by the company it keeps\" - Firth (1957)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execise 0 (0pt)\n",
    "\n",
    "\n",
    "To do the following exercises you will need certain python packages. This\n",
    "first exercise is about installing them. You will need `sklearn`, `nltk`, `numpy`,\n",
    "`gensim`. Please make sure you have installed them (by your distribution’s\n",
    "package manager, pip, anaconda, . . . ) and check your installation by trying\n",
    "to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "import numpy\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 (0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `wordspace.py` you find some convenience functions to extract a word\n",
    "cooccurrence matrix from text. Run the following script and evaluate the\n",
    "embeddings by looking at the nearest neighbors of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordspace import cooccurrence_matrix ,\\\n",
    "nearest_neighbor_loop\n",
    "\n",
    "with open('brown.txt', 'r') as f:\n",
    "    brown = f.read()\n",
    "\n",
    "matrix , vocabulary = cooccurrence_matrix(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbor_loop(matrix , vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix \n",
    "del vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to improve a basic counting model is transforming the word\n",
    "counts by, e.g., applying the square root afterwards.\n",
    "Modify the script from exercise 1.1 by using `numpy.sqrt` to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us examine the parameters of the function `cooccurrence_matrix`.\n",
    "You can modify the `window_size` and/or try a different vectorizer than\n",
    "the standard `CountVectorizer` to compute the cooccurrence scores. Try\n",
    "`sklearn.feature_extraction.text.TfidfVectorizer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_matrix(\n",
    "    text , window_size=2, max_vocab_size=20000,\n",
    "    same_word_zero=False , vectorizer=CountVectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Singular Value Decomposition (SVD) you can reduce the dimensionality\n",
    "of your embeddings. Try `sklearn.decomposition.TruncatedSVD` and\n",
    "see how your embeddings change! Consider the following usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "with open('brown.txt', 'r') as f:\n",
    "    brown = f.read()\n",
    "some_text = brown\n",
    "try:\n",
    "    C\n",
    "except:\n",
    "    C, V = cooccurrence_matrix(some_text)\n",
    "    \n",
    "svd = TruncatedSVD(\n",
    "    n_components=100, algorithm=\"randomized\",\n",
    "    n_iter=5, random_state=42, tol=0.\n",
    ")\n",
    "new_C = svd.fit_transform(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C\n",
    "del new_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_components` - desired embedding dimension\n",
    "- `algorithm` - SVD solver to use; either “arpack” or “randomized”\n",
    "- `n_iter` - number of iterations for randomized SVD solver (not used by ARPACK)\n",
    "- `random_state` - seed for pseudo-random number generator\n",
    "- `tol` -  toleranze for ARPACK. Ignored by randomized SVD solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code snippets to train your own word2vec model on the\n",
    "brown corpus (or any other large text file you have). `semantic_tests.py`\n",
    "contains some tests for your embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_tests import semantic_tests\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s: %(levelname)s: %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sent = nltk.data.load(\n",
    "    'tokenizers/punkt/english.pickle'\n",
    ")\n",
    "\n",
    "with open('brown.txt', 'r') as f:\n",
    "    sentences = sent.tokenize(f.read())\n",
    "    sentences = map(lambda s: word_tokenize(s), sentences)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences , size=100, window=5,\n",
    "    min_count=5, hs=0, negative=5,\n",
    "    cbow_mean=1, iter=5, workers=3\n",
    ")\n",
    "\n",
    "semantic_tests(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training your own word2vec model, you can also download pretrained\n",
    "embeddings and load them into `gensim.` Are they doing better in\n",
    "your `semantic_tests`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular pre-trained option is the Google News dataset model, containing 300-dimensional embeddings for 3 millions words and phrases. Download the binary file ‘GoogleNews-vectors-negative300.bin’ (1.3 GB compressed) from https://code.google.com/archive/p/word2vec/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from semantic_tests import semantic_tests\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    'path/to/GoogleNews􀀀vectors􀀀negative300.bin.gz',\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "semantic_tests(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation i pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Au1y1zTzg5L1"
   },
   "source": [
    "## Exercise 4 (2pt)\n",
    "\n",
    "\n",
    "- Train word2vec skip-gram model on sentence \"the quick brown fox jumps over the lazy dog\". Assume context window = 2, embedding_dim = 5. No preprocessing apart from tokenization.\n",
    "- Compute model output probabilities for words \"lazy\" and \"dog\". If you have trained the model correctly, the output probabilities for word \"lazy\" should be higher for words \"over\", \"the\", \"dog\" (close to 1/3 each) and lower for other words (close to 0 each). For word \"dog\", the output probabilities should be higher for words, \"the\", \"lazy\" (close to 1/2 each) and lower for other words (close to 0 each). \n",
    "- Compute dot product between the vector of word \"dog\" and the vector of word \"lazy\" (could be representation of center vector and representation of context vector) and between \"dog\" and \"brown\". Which one is higher? Why?\n",
    "\n",
    "\n",
    "You can use this tutorial https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "\n",
    "Use pytorch (or tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOKM7MX0g779"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-Fx_0DGjZDX"
   },
   "outputs": [],
   "source": [
    "sentence = \"the quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRnIu7hAB6sF"
   },
   "source": [
    "If our vocabulary is bigger, the word2vec model needs a LOT of data to obtain reasonable results. With this amount of data, the code needs to be optimized very well. Writing such code will be more suitable for a project instead of a simple exercise, therefore in the next exercise we will use [gensim](https://radimrehurek.com/gensim/), a library made for efficient training of word vectors.\n",
    "\n",
    "## * Exercise (2pt)\n",
    "\n",
    "- Use [gensim](https://radimrehurek.com/gensim/) to train a word2vec model on [OpinRank](http://kavita-ganesan.com/entity-ranking-data/). You can follow this [tutorial](https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3), but make sure you have used negative sampling.\n",
    "- Find 10 similar words to word \"dirty\" and \"canada\"\n",
    "- Check if similarity between \"dirty\" and \"dusty\" is bigger than between \"dirty\" and \"clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-WqD1xKDE0p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "lab-6-word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
